{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*RNN_Human_Detection.ipynb* <p style='text-align: right;'> <b> September 20th 2020 </b> </p>\n",
    "<p style='text-align: right;'> <b> David Diston </b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an LSTM RNN to Classify between Human and Computer Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Substantial trial-and-error tuning was performed when developing this model. The final model iteration is shown here***\n",
    "\n",
    "I have chosed to work with LSTM RNN architecture because of its success when dealing with sequential data. Since each row of data in my arrays has 88 observations (one variable/column per piano note), I initially started with a model containing 88 input nodes in an LSTM layer. After trial and error with different input nodes, I found 88 input nodes was the most optimal structure. For other models created in this project, I will show later that the optimal number of input nodes varries. Similar to tuning the number of input nodes, the number of hidden layers, and nodes within those layers were adjusted and tested to find the best model architecture for this problem. \n",
    "\n",
    "Based on this tuning, the most optimal model for this project was deterined to be 88 LSTM input nodes, followed by one hidden LSTM layer also with 88 nodes, a hidden dense layer with 1408 nodes, and the final output layer with 2 nodes and softmax activation. I have also included dropout to try and mitigate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries for modeling\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have created a function to load my data that can be called for the training, validation, and test sets\n",
    "# This function will take in the list of classes, and the directory where to find the data\n",
    "def training_set(cat, datdir):\n",
    "    # Using the category labels, I can iterate through each folder\n",
    "    for category in cat:\n",
    "        path = os.path.join(datdir, category)\n",
    "        class_num = categories.index(category)\n",
    "        # Using the path, and index of the category, I can append both together in a training_data list\n",
    "        for array in os.listdir(path):\n",
    "            # Here I am going to use try, to avoid breaking the loop if it comes accross an issue loading a particular array\n",
    "            try:\n",
    "                midi_array = np.load(os.path.join(path, array))\n",
    "                training_data.append([midi_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Data\n",
    "datadirectory = 'C:/Users/David Diston/Documents/CapstonePythonScripts/HumComp/HumComp_Data'\n",
    "categories = ['Computer', 'Human']\n",
    "\n",
    "# I'll instantiate my training_data list\n",
    "training_data = []\n",
    "\n",
    "# I will pass the path and categories into my data import function\n",
    "training_set(categories, datadirectory)\n",
    "\n",
    "# I am going to shuffle the data for more accurate training\n",
    "random.shuffle(training_data)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# For every array/class pair in the data list, I will append the array and class to the X and y variable respectively\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Validation Data\n",
    "datadirectory = 'C:/Users/David Diston/Documents/CapstonePythonScripts/HumComp/HumComp_Validation_Set_Data'\n",
    "categories = ['Computer', 'Human']\n",
    "\n",
    "# I am going to use the same data import function I created, so I will reinstantiate the list\n",
    "training_data = []\n",
    "\n",
    "# Again I will pass in the categories/class labels and data path for teh validation data\n",
    "training_set(categories, datadirectory)\n",
    "\n",
    "# I will shuffle the validation data\n",
    "random.shuffle(training_data)\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "# Once again I will break apart the array/class pairs and attach them to the validation variables\n",
    "for features, label in training_data:\n",
    "    X_val.append(features)\n",
    "    y_val.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will convert all data lists to arrays, and reshape to the correct shape for network input\n",
    "X_train = np.array(X).reshape(-1, 100, 88)\n",
    "y_train = np.array(y)\n",
    "X_val = np.array(X_val).reshape(-1, 100, 88)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Here I will normalize all the data to improve training\n",
    "# Since all note velocities are between 0-127, I can divide by 127 to convert all velocities to between 0 and 1\n",
    "X_train = X_train / 127\n",
    "X_val = X_val / 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will instantiate my model\n",
    "HumCom_model = Sequential()\n",
    "\n",
    "# Through trial and error I have found 88 input nodes to be optimal\n",
    "# I am also returning a sequence to be passed to my next hidden LSTM layer\n",
    "# Dropout is used after each layer to prevent overfitting\n",
    "HumCom_model.add(LSTM(88, input_shape = (X_train.shape[1:]), activation = 'relu', return_sequences = True))\n",
    "HumCom_model.add(Dropout(0.1))\n",
    "\n",
    "# My first hidden layer also with relu activation\n",
    "# A sequence output is not required here since the next layer is dense\n",
    "HumCom_model.add(LSTM(88, activation='relu'))\n",
    "HumCom_model.add(Dropout(0.1))\n",
    "\n",
    "# Trial and error by factors of 44 nodes found that 1408 nodes in the hidden dense layer was optimal\n",
    "# I experimented with more and less hidden layers and found this arrangement to be optimal\n",
    "HumCom_model.add(Dense(1408, activation='relu'))\n",
    "HumCom_model.add(Dropout(0.1))\n",
    "\n",
    "# The final output of the model will predict between the two classes\n",
    "HumCom_model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 88)           62304     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 88)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 88)                62304     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1408)              125312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1408)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 2818      \n",
      "=================================================================\n",
      "Total params: 252,738\n",
      "Trainable params: 252,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# I am going to use the adam optimizer with an average learning rate and decay\n",
    "# I also had issues with gradient explosion, so I introduce value clipping of 0.5\n",
    "opt = Adam(lr = 0.001, decay = 0.00001, clipvalue=0.5)\n",
    "\n",
    "# Here I compile my model with the loss function, the optimizer, an accuracy metric\n",
    "HumCom_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer = opt,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Prior to training I print out a summary of the model I have created\n",
    "HumCom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 30520976.0000 - accuracy: 0.5824\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.64650, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 30520976.0000 - accuracy: 0.5824 - val_loss: 0.6415 - val_accuracy: 0.6465\n",
      "Epoch 2/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.7099\n",
      "Epoch 00002: val_accuracy improved from 0.64650 to 0.73900, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 65ms/step - loss: 0.5623 - accuracy: 0.7099 - val_loss: 3730435.2500 - val_accuracy: 0.7390\n",
      "Epoch 3/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 6683548.0000 - accuracy: 0.7229\n",
      "Epoch 00003: val_accuracy did not improve from 0.73900\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 6683548.0000 - accuracy: 0.7229 - val_loss: 0.7006 - val_accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 82.9146 - accuracy: 0.5506\n",
      "Epoch 00004: val_accuracy did not improve from 0.73900\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 82.9146 - accuracy: 0.5506 - val_loss: 0.6806 - val_accuracy: 0.5660\n",
      "Epoch 5/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.5906 - accuracy: 0.6862\n",
      "Epoch 00005: val_accuracy did not improve from 0.73900\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 0.5906 - accuracy: 0.6862 - val_loss: 0.5570 - val_accuracy: 0.7205\n",
      "Epoch 6/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.4993 - accuracy: 0.7622\n",
      "Epoch 00006: val_accuracy improved from 0.73900 to 0.75800, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 0.4993 - accuracy: 0.7622 - val_loss: 0.5131 - val_accuracy: 0.7580\n",
      "Epoch 7/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.4448 - accuracy: 0.7974\n",
      "Epoch 00007: val_accuracy improved from 0.75800 to 0.77450, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 0.4448 - accuracy: 0.7974 - val_loss: 0.4725 - val_accuracy: 0.7745\n",
      "Epoch 8/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.4079 - accuracy: 0.8164\n",
      "Epoch 00008: val_accuracy improved from 0.77450 to 0.79950, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 0.4079 - accuracy: 0.8164 - val_loss: 0.4469 - val_accuracy: 0.7995\n",
      "Epoch 9/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.3795 - accuracy: 0.8315\n",
      "Epoch 00009: val_accuracy did not improve from 0.79950\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 0.3795 - accuracy: 0.8315 - val_loss: 0.4295 - val_accuracy: 0.7940\n",
      "Epoch 10/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.3626 - accuracy: 0.8424\n",
      "Epoch 00010: val_accuracy improved from 0.79950 to 0.82900, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 0.3626 - accuracy: 0.8424 - val_loss: 0.4205 - val_accuracy: 0.8290\n",
      "Epoch 11/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.8644\n",
      "Epoch 00011: val_accuracy improved from 0.82900 to 0.83950, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 0.3229 - accuracy: 0.8644 - val_loss: 0.3704 - val_accuracy: 0.8395\n",
      "Epoch 12/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.2762 - accuracy: 0.8890\n",
      "Epoch 00012: val_accuracy improved from 0.83950 to 0.86050, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 0.2762 - accuracy: 0.8890 - val_loss: 0.3598 - val_accuracy: 0.8605\n",
      "Epoch 13/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.3793 - accuracy: 0.8528\n",
      "Epoch 00013: val_accuracy improved from 0.86050 to 0.90200, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 0.3793 - accuracy: 0.8528 - val_loss: 0.2588 - val_accuracy: 0.9020\n",
      "Epoch 14/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.2045 - accuracy: 0.9285\n",
      "Epoch 00014: val_accuracy improved from 0.90200 to 0.91850, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 65ms/step - loss: 0.2045 - accuracy: 0.9285 - val_loss: 0.2310 - val_accuracy: 0.9185\n",
      "Epoch 15/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9542\n",
      "Epoch 00015: val_accuracy improved from 0.91850 to 0.94400, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 65ms/step - loss: 0.1281 - accuracy: 0.9542 - val_loss: 0.1521 - val_accuracy: 0.9440\n",
      "Epoch 16/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.6357 - accuracy: 0.9551\n",
      "Epoch 00016: val_accuracy improved from 0.94400 to 0.94550, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 1.6357 - accuracy: 0.9551 - val_loss: 0.1586 - val_accuracy: 0.9455\n",
      "Epoch 17/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 33513388.0000 - accuracy: 0.8952\n",
      "Epoch 00017: val_accuracy did not improve from 0.94550\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 33513388.0000 - accuracy: 0.8952 - val_loss: 22.2019 - val_accuracy: 0.7990\n",
      "Epoch 18/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.6028 - accuracy: 0.9317\n",
      "Epoch 00018: val_accuracy improved from 0.94550 to 0.97100, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 0.6028 - accuracy: 0.9317 - val_loss: 0.0925 - val_accuracy: 0.9710\n",
      "Epoch 19/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 13.5659 - accuracy: 0.9763\n",
      "Epoch 00019: val_accuracy improved from 0.97100 to 0.97800, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 13.5659 - accuracy: 0.9763 - val_loss: 0.0688 - val_accuracy: 0.9780\n",
      "Epoch 20/20\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9837\n",
      "Epoch 00020: val_accuracy improved from 0.97800 to 0.98700, saving model to Models/HumCom_Model.hdf5\n",
      "320/320 [==============================] - 21s 66ms/step - loss: 0.0510 - accuracy: 0.9837 - val_loss: 0.0492 - val_accuracy: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b768752a08>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By including this checkpoint I can save a copy of the weights from the best performing epoch\n",
    "# I will identify the best performing epoch by monitoring the validation accuracy\n",
    "checkpoint = ModelCheckpoint('Models/HumCom_Model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Now time to fit the model\n",
    "# I set epochs to 20 since I found that after 20 epochs the model tends to start overfitting and there is no further loss reduction\n",
    "HumCom_model.fit(X_train, y_train,\n",
    "               batch_size = 50,\n",
    "               epochs = 20,\n",
    "               validation_data=(X_val, y_val),\n",
    "               callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\David Diston\\anaconda3\\envs\\bs_capstone\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: Models/HumCom_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# In addition to saving the best weights with callback, I will also save a copy of the full model I created\n",
    "HumCom_model.save('Models/HumCom_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I import the libraries and metrics for evaluating the success of my model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The standard confusion matrix looks terrible and is difficult to read\n",
    "# The below function will help visually display the confusion matrix results\n",
    "# I got the idea for this function from a previous LSTM study, and have adapted it to my purposes below\n",
    "def plot_confusion_matrix(cm, classes, cmap=plt.cm.Blues):\n",
    "\n",
    "    # Here I am calling the plot of the confusion matrix with the Blue cmap as standard\n",
    "    # I found that the 'nearest' interpolation produced the clearest results\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    # I will also include the colour bar for reference\n",
    "    plt.colorbar()\n",
    "    # I set the numer of ticks on the x and y axis to the number of classes which will be useful for multiclass models\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    # Here I am going to rotate the x-axis ticks 45 degrees to avoid crouding with the x-axis label\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    # The threshold is half of the maximum value present in the confusion matrix\n",
    "    thresh = cm.max() / 2\n",
    "    # This for loop places the correct numerical text in the correct box, and was borrowed from a previous project\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    # Finally I will include axis labels for clarity\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code was inspired by [this project.](https://colab.research.google.com/drive/1ISfhxFDntfOos7cOeT7swduSqzLEqyFn)\n",
    "\n",
    "Only after building this code did I realize that `sklearn.metrics` has a `plot_confusion_matrix` function that works in a very similar way. However to accurately show my process I have included my own function as I originally created it (with inspiration as referenced).\n",
    "\n",
    "Now I can load the test data in the same way I loaded the training and validation data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "datadirectory = 'C:/Users/David Diston/Documents/CapstonePythonScripts/HumComp/HumComp_Test_Set_Data'\n",
    "categories = ['Computer', 'Human']\n",
    "\n",
    "# Similar to above I will instantiate the data list and use the data loading function I created\n",
    "training_data = []\n",
    "    \n",
    "training_set(categories, datadirectory)\n",
    "\n",
    "# I will again shuffle the data, though this process is  not required for the testing data\n",
    "random.shuffle(training_data)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# Once again I will split the arrays and their class labels into their respective variables\n",
    "for features, label in training_data:\n",
    "    X_test.append(features)\n",
    "    y_test.append(label)\n",
    "\n",
    "# I will convert to array and reshape the data\n",
    "X_test = np.array(X_test).reshape(-1, 100, 88)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# I will also normalize the test data to values between 0 and 1\n",
    "X_test = X_test / 127"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I can use the test data to make predictions using the model built above, and plot the confusion matrix for a visual representation of the classification success of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I will use the predict function to get predictions based on the test set arrays\n",
    "results = HumCom_model.predict(X_test, batch_size = 10)\n",
    "\n",
    "# I will convert the class probabilities into actual class predictions\n",
    "predicted_classes = np.argmax(results, axis=1)\n",
    "\n",
    "# Names of predicted classes\n",
    "class_names = ['Computer', 'Human']\n",
    "\n",
    "# Using sklearn and the true and predicted class labels I will create a confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, predicted_classes)\n",
    "\n",
    "# Finally I will plot the confusion matrix using the above function\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names)\n",
    "plt.title('Human Detection Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1000\n",
      "           1       0.99      0.99      0.99      1000\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.99      0.99      0.99      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I will also print a classification report of the class predictions to better understand the errors\n",
    "CR = classification_report(y_test, predicted_classes)\n",
    "print(CR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall I am very happy with the high validation and test accuracy I was able to achieve with my model. These results were not entirely unexpected. Even though it is often difficult for the human ear to differentiate bewteen human-performed and computer-performed midi files, as has been discussed so far these types of files are quite different at the data level. \n",
    "\n",
    "Now that I have established that computer and human performed midi files can be identified, I will build a model that attempts to classify the composer of a piece of music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: right;'> <b> Next Step: </b> Preprocess data required to build a Composer classification model - <em> Composer_Model_Data_Preparation.ipynb </em> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bs_capstone]",
   "language": "python",
   "name": "conda-env-bs_capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
