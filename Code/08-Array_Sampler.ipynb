{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Array_Sampler.ipynb* <p style='text-align: right;'> <b> September 20th 2020 </b> </p>\n",
    "<p style='text-align: right;'> <b> David Diston </b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly Sample Arrays to Build Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Here I will randomly sample each array I have created to make 100 row clips to be used in training and testing***\n",
    "\n",
    "Each of the Midi arrays I have created are ~a couple hundred or thousand rows long. This is a substantial length of data to turn into a tensor and pass through a Neural Network. Additionally, each of these arrays have different lengths, which need to be standardized before training. To solve both of these problems, I will be creating 100 row samples randomly from the collection of arrays I have sofar created. In order to prevent data leakage into the validation and test sets, these sets have already been split out of the training set, and will each be sampled independently.\n",
    "Based on the number of arrays I have created from the original midi files, and the average length of these arrays, I have determined that 20,000 array samples will be a representative selection of the original data. 16,000 will be used for training (8,000 for each class), and 2,000 each for the test and validations sets (1,000 for each class in each set) will be created. While the class sizes for this model are equal, this sampling method will also serve to equalize class sets for later models in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 16000 array clips.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "# I will iterate over both the human and computer folders\n",
    "for folder in os.listdir('HumComp/HumComp_Array/'):\n",
    "\n",
    "    # For 8,000 clips in each class I will randomly select one array file from the class\n",
    "    for i in range(0, 8000):\n",
    "        file = random.choice(os.listdir(f'HumComp/HumComp_Array/{folder}/'))\n",
    "        name = ''\n",
    "        # Load each array\n",
    "        array = np.load(f'HumComp/HumComp_Array/{folder}/{file}')\n",
    "\n",
    "        # I will randomly select a 100 row length sample from each array for modeling\n",
    "        # I will create a length variable 100 rows less than the length of the file from which to select a starting row\n",
    "        length = (len(array) - 100)\n",
    "        start = random.randint(0, length)\n",
    "        end = start + 100\n",
    "        # The array clip will be saved as the 100 rows from the start -> end row\n",
    "        array = array[start: end]\n",
    "\n",
    "        # I will create a new file name and save the array clip\n",
    "        name = 'CLIP--' + str(count) + '--' + file\n",
    "        np.save(f'HumComp/HumComp_Data/{folder}/{name}', array)\n",
    "\n",
    "        count += 1    \n",
    "        print(f'Processed {count} array clips.', end = '\\r')\n",
    "        \n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 array clips.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "# I will iterate over both the human and computer folders\n",
    "for folder in os.listdir('HumComp/HumComp_Test_Set_Arrays/'):\n",
    "\n",
    "    # For 1,000 clips in each class I will randomly select one array file from the class\n",
    "    for i in range(0, 1000):\n",
    "        file = random.choice(os.listdir(f'HumComp/HumComp_Test_Set_Arrays/{folder}/'))\n",
    "        name = ''\n",
    "        # Load each array\n",
    "        array = np.load(f'HumComp/HumComp_Test_Set_Arrays/{folder}/{file}')\n",
    "\n",
    "        # I will randomly select a 100 row length sample from each array for modeling\n",
    "        # I will create a length variable 100 rows less than the length of the file from which to select a starting row\n",
    "        length = (len(array) - 100)\n",
    "        start = random.randint(0, length)\n",
    "        end = start + 100\n",
    "        array = array[start: end]\n",
    "\n",
    "        # I will create a new file name and save the array clip\n",
    "        name = 'TEST_CLIP--' + str(count) + '--' + file\n",
    "        np.save(f'HumComp/HumComp_Test_Set_Data/{folder}/{name}', array)\n",
    "\n",
    "        count += 1    \n",
    "        print(f'Processed {count} array clips.', end = '\\r')\n",
    "        \n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 array clips.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for folder in os.listdir('HumComp/HumComp_Validation_Set_Arrays/'):\n",
    "\n",
    "    # For 1,000 clips in each class I will randomly select one array file from the class\n",
    "    for i in range(0, 1000):\n",
    "        file = random.choice(os.listdir(f'HumComp/HumComp_Validation_Set_Arrays/{folder}/'))\n",
    "        name = ''\n",
    "        # Load each array\n",
    "        array = np.load(f'HumComp/HumComp_Validation_Set_Arrays/{folder}/{file}')\n",
    "\n",
    "        # I will randomly select a 100 row length sample from each array for modeling\n",
    "        # I will create a length variable 100 rows less than the length of the file from which to select a starting row\n",
    "        length = (len(array) - 100)\n",
    "        start = random.randint(0, length)\n",
    "        end = start + 100\n",
    "        array = array[start: end]\n",
    "\n",
    "        # I will create a new file name and save the array clip\n",
    "        name = 'VAL_CLIP--' + str(count) + '--' + file\n",
    "        np.save(f'HumComp/HumComp_Validation_Set_Data/{folder}/{name}', array)\n",
    "\n",
    "        count += 1    \n",
    "        print(f'Processed {count} array clips.', end = '\\r')\n",
    "        \n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have my training, validation, and test sets organized into coherent file structures, it is time to start modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: right;'> <b> Next Step: </b> Build RNN Binary Classification Model - <em> RNN_Human_Detection.ipynb </em> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bs_capstone]",
   "language": "python",
   "name": "conda-env-bs_capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
